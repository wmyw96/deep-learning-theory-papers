# Deep Learning Theory Papers

## Empirical Investigation

### Similarity of Representations

- Convergent Learning: Do Different Neural Networks Learn the Same Representations? ([paper](https://arxiv.org/abs/1511.07543))
- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability ([paper](http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-understanding-and-improvement))
- Insights on representational similarity in neural networks with canonical correlation ([paper](http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation))
- Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation ([paper](https://arxiv.org/abs/1810.11750))
- Similarity of Neural Network Representations Revisited ([paper](https://arxiv.org/abs/1905.00414))
- Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains ([paper](https://arxiv.org/pdf/1906.01539))
- Subspace Match Probably Does Not Accurately Assess the Similarity of Learned Representations ([paper](https://arxiv.org/abs/1901.00884))
- Deep Ensembles: A Loss Landscape Perspective ([paper](https://arxiv.org/abs/1912.02757))

### Mode Connectivity

- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs ([paper](https://arxiv.org/abs/1802.10026))
- Large Scale Structure of Neural Network Loss Landscapes ([paper](https://arxiv.org/abs/1906.04724))
- Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets ([paper](http://papers.nips.cc/paper/9602-explaining-landscape-connectivity-of-low-cost-solutions-for-multilayer-nets))
- On Connected Sublevel Sets in Deep Learning ([paper](https://arxiv.org/abs/1901.07417))
- Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape ([paper](https://arxiv.org/abs/1907.02911))
- Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks ([paper](https://arxiv.org/pdf/1912.10095))

### Lottery Ticket Hypothesis

- The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks ([paper](https://arxiv.org/abs/1803.03635))
- Linear Mode Connecivity and the Lottery Ticket Hypothesis ([paper](https://arxiv.org/abs/1912.05671))

### Evaluation of Theory

- Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory ([paper](https://arxiv.org/abs/1910.00359))
- Disentangling feature and lazy learning in deep neural networks: an empirical study ([paper](https://arxiv.org/abs/1906.08034))

## Training Dynamics

### Mean Field View (Active Training)

- A Mean Field View of the Landscape of Two-Layers Neural Networks ([paper](https://arxiv.org/abs/1804.06561))
- On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport ([paper](http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport))
- Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel ([paper](https://arxiv.org/abs/1810.05369v3))
- Mean-Field Langevin Dynamics and Energy Landscape of Neural Networks ([paper](https://arxiv.org/abs/1905.07769))
- A Mean-Field Limit for Certain Deep Neural Networks ([paper](https://arxiv.org/abs/1906.00193))
- Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks ([paper](https://arxiv.org/abs/1902.02880))

### Neural Tangent Kernel (Lazy Training)

- On Lazy Training in Differentiable Programming ([paper](https://arxiv.org/abs/1812.07956))
- Neural Tangent Kernel: Convergence and Generalization in Neural Networks ([paper](https://arxiv.org/abs/1806.07572))

## Loss Landscape Analysis

### Spurious Valley / Local Minima

- Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes ([paper](http://jmlr.org/papers/v20/18-674.html))
- Local Minima in Training of Neural Networks ([paper](https://arxiv.org/abs/1611.06310))
- Small Nonlinearities in Activation Functions Create Bad Local Minima in Neural Networks ([paper](https://arxiv.org/abs/1802.03487))

## Analysis of Operators

### Batch Normalization

- How Does Batch Normalization Help Optimization? ([paper](https://arxiv.org/abs/1805.11604))

### Residual Connection

- Are ResNets Provably Better than Linear Predictors? ([paper](https://arxiv.org/abs/1907.03922))

## Generative Models

### GAN Training Dynamics

- On the Limitations of First-Order Approximation in GAN Dynamics ([paper](http://proceedings.mlr.press/v80/li18d.html))
- Gradient descent GAN optimization is locally stable ([paper](https://arxiv.org/abs/1706.04156))

## AutoEncoder

- Loss Landscapes of Regularized Linear Autoencoders ([paper](https://arxiv.org/pdf/1901.08168))
