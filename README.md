# Deep Learning Theory Papers

## Empirical Investigation

### Similarity of Representations

- Convergent Learning: Do Different Neural Networks Learn the Same Representations? ([paper](https://arxiv.org/abs/1511.07543))
- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability ([paper](http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-understanding-and-improvement))
- Insights on representational similarity in neural networks with canonical correlation ([paper](http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation))
- Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation ([paper](https://arxiv.org/abs/1810.11750))
- Similarity of Neural Network Representations Revisited ([paper](https://arxiv.org/abs/1905.00414))
- Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains ([paper](https://arxiv.org/pdf/1906.01539))
- Subspace Match Probably Does Not Accurately Assess the Similarity of Learned Representations ([paper](https://arxiv.org/abs/1901.00884))
- Deep Ensembles: A Loss Landscape Perspective ([paper](https://arxiv.org/abs/1912.02757))

### Mode Connectivity

- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs ([paper](https://arxiv.org/abs/1802.10026))

### Evaluation of Theory

- Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory ([paper](https://arxiv.org/abs/1910.00359))

## Training Dynamics

### Mean Field View (Active Training)

- A Mean Field View of the Landscape of Two-Layers Neural Networks ([paper](https://arxiv.org/abs/1804.06561))
- On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport ([paper](http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport))
- Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel ([paper](https://arxiv.org/abs/1810.05369v3))
- Mean-Field Langevin Dynamics and Energy Landscape of Neural Networks ([paper](https://arxiv.org/abs/1905.07769))
- A Mean-Field Limit for Certain Deep Neural Networks ([paper](https://arxiv.org/abs/1906.00193))
- Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks ([paper](https://arxiv.org/abs/1902.02880))

### Neural Tangent Kernel (Lazy Training)
