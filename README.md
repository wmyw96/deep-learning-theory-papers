# Deep Learning Theory Papers

## Empirical Investigation

### Similarity of Representations

- Convergent Learning: Do Different Neural Networks Learn the Same Representations? (2015, [paper](https://arxiv.org/abs/1511.07543))
- SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (2017, [paper](http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-understanding-and-improvement))
- Insights on representational similarity in neural networks with canonical correlation (2018, [paper](http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation))
- Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation (2018, [paper](https://arxiv.org/abs/1810.11750))
- Subspace Match Probably Does Not Accurately Assess the Similarity of Learned Representations (2019, [paper](https://arxiv.org/abs/1901.00884))
- Similarity of Neural Network Representations Revisited (2019, [paper](https://arxiv.org/abs/1905.00414))
- Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains (2019, [paper](https://arxiv.org/pdf/1906.01539))
- Deep Ensembles: A Loss Landscape Perspective (2019, [paper](https://arxiv.org/abs/1912.02757))

### Mode Connectivity

- Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs (2018, [paper](https://arxiv.org/abs/1802.10026))
- Large Scale Structure of Neural Network Loss Landscapes (2019, [paper](https://arxiv.org/abs/1906.04724))
- Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets (2019, [paper](http://papers.nips.cc/paper/9602-explaining-landscape-connectivity-of-low-cost-solutions-for-multilayer-nets))
- On Connected Sublevel Sets in Deep Learning (2019, [paper](https://arxiv.org/abs/1901.07417))
- Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape (2019, [paper](https://arxiv.org/abs/1907.02911))
- Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks (2019, [paper](https://arxiv.org/pdf/1912.10095))

### Lottery Ticket Hypothesis

- The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (2018, [paper](https://arxiv.org/abs/1803.03635))
- Linear Mode Connecivity and the Lottery Ticket Hypothesis (2019, [paper](https://arxiv.org/abs/1912.05671))

### Evaluation of Theory

- Disentangling feature and lazy learning in deep neural networks: an empirical study (2019, [paper](https://arxiv.org/abs/1906.08034))
- Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory (2019, [paper](https://arxiv.org/abs/1910.00359))

## Training Dynamics

### Mean Field View (Active Training)

- A Mean Field View of the Landscape of Two-Layers Neural Networks (2018, [paper](https://arxiv.org/abs/1804.06561))
- On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport (2018, [paper](http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport))
- Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel (2018, [paper](https://arxiv.org/abs/1810.05369v3))
- Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks (2019, [paper](https://arxiv.org/abs/1902.02880))
- Mean-Field Langevin Dynamics and Energy Landscape of Neural Networks (2019, [paper](https://arxiv.org/abs/1905.07769))
- A Mean-Field Limit for Certain Deep Neural Networks (2019, [paper](https://arxiv.org/abs/1906.00193))

### Neural Tangent Kernel (Lazy Training)

- Neural Tangent Kernel: Convergence and Generalization in Neural Networks (2018, [paper](https://arxiv.org/abs/1806.07572))
- On Lazy Training in Differentiable Programming (2018, [paper](https://arxiv.org/abs/1812.07956))

## Loss Landscape Analysis

### Spurious Valley / Local Minima

- Local Minima in Training of Neural Networks (2016, [paper](https://arxiv.org/abs/1611.06310))
- Small Nonlinearities in Activation Functions Create Bad Local Minima in Neural Networks (2018, [paper](https://arxiv.org/abs/1802.03487))
- Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes (2019, [paper](http://jmlr.org/papers/v20/18-674.html))

## Analysis of Operators

### Batch Normalization

- How Does Batch Normalization Help Optimization? (2018, [paper](https://arxiv.org/abs/1805.11604))

### Residual Connection

- Are ResNets Provably Better than Linear Predictors? (2019,[paper](https://arxiv.org/abs/1907.03922))

## Generative Models

### GAN Training Dynamics

- Gradient descent GAN optimization is locally stable (2017, [paper](https://arxiv.org/abs/1706.04156))
- On the Limitations of First-Order Approximation in GAN Dynamics (2018, [paper](http://proceedings.mlr.press/v80/li18d.html))

## AutoEncoder

- Loss Landscapes of Regularized Linear Autoencoders (2019, [paper](https://arxiv.org/pdf/1901.08168))
